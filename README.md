# ğŸš€ Projet Spark-Docker

## ğŸ“˜ Description

Ce projet a pour objectif de **dÃ©ployer un environnement Apache Spark** Ã  lâ€™aide de **Docker** et **Docker Compose**, afin de faciliter le dÃ©veloppement, lâ€™exÃ©cution et lâ€™analyse de traitements distribuÃ©s en **PySpark**.
Il contient des scripts Python, des notebooks dâ€™analyse et des fichiers de configuration pour automatiser le dÃ©ploiement et lâ€™exÃ©cution dâ€™un cluster Spark localement.


## ğŸ§© Structure du projet

```
SPARK-DOCKER/
â”‚
â”œâ”€â”€ .benv/                    # Environnement virtuel Python (optionnel)
â”‚
â”œâ”€â”€ app/
â”‚   â””â”€â”€ mon_script.py         # Script Python exÃ©cutÃ© sur Spark
â”‚
â”œâ”€â”€ notebooks/
â”‚   â””â”€â”€ analyse_spark.ipynb   # Notebook Jupyter pour lâ€™analyse des donnÃ©es via PySpark
â”‚
â”œâ”€â”€ docker-compose.yaml       # Configuration du cluster Spark (master + workers)
â”‚
â”œâ”€â”€ Dockerfile                # Image personnalisÃ©e pour Spark et dÃ©pendances Python
â”‚
â”œâ”€â”€ execution_hote.sh         # Script shell pour exÃ©cuter Spark depuis lâ€™hÃ´te (passez en argument votre fichier .py dans app)
â”‚
â””â”€â”€ README.md                 # Documentation du projet
```

---

## âš™ï¸ FonctionnalitÃ©s principales

* DÃ©ploiement automatique dâ€™un cluster **Spark Master** et **Workers** via Docker Compose.
* ExÃ©cution de scripts **PySpark** (localement ou dans le conteneur).
* Lancement et visualisation dâ€™analyses interactives via **Jupyter Notebook**.
* Script Shell (`execution_hote.sh`) pour exÃ©cuter des tÃ¢ches Spark directement depuis la machine hÃ´te.

---

## ğŸ§± PrÃ©requis

Avant de commencer, assure-toi dâ€™avoir installÃ© :

* [Docker](https://docs.docker.com/get-docker/)
* [Docker Compose](https://docs.docker.com/compose/)
* (Optionnel) [Python 3.x](https://www.python.org/) et `pip` pour exÃ©cuter les scripts locaux

---

## ğŸš€ DÃ©marrage rapide

### 1ï¸âƒ£ Construire et lancer le cluster

```bash
docker-compose up --build
```

### 2ï¸âƒ£ AccÃ©der Ã  lâ€™interface

Une fois le cluster lancÃ©, rends-toi sur :

```
http://localhost:8888
```

ou, selon ta configuration :

```
http://localhost:8080
```

pour accÃ©der au **Jupyter Notebook** ou Ã  lâ€™interface **Spark Master**.
Tu peux y exÃ©cuter ton code directement.

---

### ğŸ§¯ ArrÃªter le cluster

```bash
docker-compose down -v
```

### ğŸ”¥ Relancer le cluster

```bash
docker-compose up -d
```

---

### âš¡ ExÃ©cuter un script `.py` prÃ©sent dans `app/`

#### Donner les permissions dâ€™exÃ©cution au script :

```bash
chmod +x execution_hote.sh
```

#### Lancer ton script :

```bash
./execution_hote.sh mon_script.py
```
Il est Ã  noter que le fichier .py doit exister dans le dossier /app 
apres un `docker compose up -d`, on peut donner les droits d'execution le `chmod` et executer le fichier `./execution_hote.sh mon_du_fichier.py`.
---

Le cluster dÃ©marre avec :

* Un **Spark Master** accessible sur `http://localhost:8080`
* Un **Jupyter Notebook** (si configurÃ© dans le Dockerfile)
* Des **Workers** prÃªts Ã  exÃ©cuter les tÃ¢ches PySpark

---

## ğŸ§ª Utilisation du Notebook

Pour explorer les donnÃ©es :

1. Ouvre le notebook `notebooks/analyse_spark.ipynb`
2. Lance le serveur Jupyter Ã  lâ€™intÃ©rieur du conteneur (si configurÃ©)
3. AccÃ¨de Ã  lâ€™interface via ton navigateur


## ğŸ§° Personnalisation

* Modifie le **Dockerfile** pour ajouter des dÃ©pendances Python supplÃ©mentaires.
* Ajuste le **docker-compose.yaml** pour modifier le nombre de workers, les ports ou les volumes.
* Remplace `mon_script.py` par tout autre job Spark.

---

## ğŸ§¹ Nettoyage

Pour arrÃªter et supprimer les conteneurs et volumes :

```bash
docker-compose down -v
```

---

## ğŸ“° Ã€ venir

Un **blog sur Medium** est en cours de rÃ©daction pour expliquer le fonctionnement complet du projet.
ğŸ“¢ Abonne-toi Ã  mon **LinkedIn** pour ne rien rater !
Merci pour ton intÃ©rÃªt ğŸ™Œ

---

## ğŸ‘¨â€ğŸ’» Auteur

**Ben Tamou**
Projet dâ€™expÃ©rimentation et de dÃ©ploiement de Spark avec Docker
ğŸ“… *Octobre 2025*

